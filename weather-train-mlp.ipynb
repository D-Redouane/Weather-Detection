{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vl8ni9ng11b_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Sequential\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jaQRsNkT2hSV",
        "outputId": "eb236a2f-0abb-4d82-e12b-4c996c4ba2d2"
      },
      "outputs": [],
      "source": [
        "# # ðŸ”´ðŸ”´ðŸ”´\n",
        "\n",
        "# # verify that the directory exists\n",
        "# wetherdir = '/content/sample_data/weather-dataset/'\n",
        "\n",
        "# if not os.path.exists(wetherdir):\n",
        "#      os.makedirs(wetherdir)\n",
        "#      print(\"The weather dataset repository has been created.\")\n",
        "# else:\n",
        "#      print(\"The weather dataset repository already exists.\")\n",
        "\n",
        "# # get the url from here after hiting the download button of the data set on kaggel https://www.kaggle.com/datasets/jehanbhathena/weather-dataset\n",
        "\n",
        "# # Download the file to the specified directory\n",
        "# !wget -O /content/sample_data/weather-dataset/archive.zip 'https://storage.googleapis.com/kaggle-data-sets/1715246/2854929/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20240501%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20240501T063340Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=21a2b2a4df08670f3758c19a71c502108618b2e86e1fb5be358dd64eab7df50355ef699a3303ef6fe03928a7507c08a32c2ab82a52a499c17690ef7f3a9cfab6b585a2f9973adfa02015812a87c8706943455a2c061c0efaaade1a872af9a266b06b31d19ae73cde1c441eec84d1460b37028f942a2ad45d6eba12e1b6b491fe85aa645adb3512f88f70a38b51306e58ebcb7d86501272f90216b5c71448e3f4a7490052f897e9edf3d240b53d5e83adfd04bc3b356ebda80edcb1ec7fa782e81ca9af731c89af4064812818e17caa39ae7e01c537eca56e79f0bb94b846fb0e5262c024659e815dc1682299e4c97bf8ebd4603efced737533a405f1e1b8d64c'\n",
        "\n",
        "# # Unzip the downloaded file to the specified directory\n",
        "# !unzip /content/sample_data/weather-dataset/archive.zip -d /content/sample_data/weather-dataset/\n",
        "\n",
        "# datadir = '/content/sample_data/weather-dataset/dataset'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2X7d_fJI3R4w",
        "outputId": "7d589635-0f0b-4e6c-e4d4-d3c6763b79c9"
      },
      "outputs": [],
      "source": [
        "# Define data augmentation and preprocessing for training and validation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    horizontal_flip=True,\n",
        "    zoom_range=0.2,\n",
        "    rotation_range=20,\n",
        "    shear_range=0.2,\n",
        "    validation_split=0.2  # 20% for validation\n",
        ")\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)  # Only rescaling for test data\n",
        "\n",
        "# Create the data generators\n",
        "train_data = train_datagen.flow_from_directory(\n",
        "    datadir,\n",
        "    target_size=(224, 224),  # Size expected by VGG16\n",
        "    batch_size=64,\n",
        "    class_mode='categorical',\n",
        "    subset='training'  # Use the training subset\n",
        ")\n",
        "\n",
        "val_data = train_datagen.flow_from_directory(\n",
        "    datadir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=64,\n",
        "    class_mode='categorical',\n",
        "    subset='validation'  # Use the validation subset\n",
        ")\n",
        "\n",
        "test_data = test_datagen.flow_from_directory(\n",
        "    datadir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=64,\n",
        "    class_mode='categorical'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9c7BKSp1zxC",
        "outputId": "f6dab6a4-c51d-4ac5-fbf2-eb9436cf5d2a"
      },
      "outputs": [],
      "source": [
        "# Load the pre-trained VGG16 model without the top layers (fully connected layers)\n",
        "base_model = VGG16(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tigSd__N17Ic"
      },
      "outputs": [],
      "source": [
        "# Freeze the base model layers\n",
        "base_model.trainable = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FaqT343z1-C2"
      },
      "outputs": [],
      "source": [
        "# Create a Sequential MLP-like model using the pre-trained base\n",
        "model = Sequential([\n",
        "    base_model,  # Pre-trained base model\n",
        "    layers.Flatten(),  # Flatten the output of the base model\n",
        "    layers.Dense(256, activation='relu'),  # First dense layer\n",
        "    layers.Dropout(0.5),  # Dropout for regularization\n",
        "    layers.Dense(128, activation='relu'),  # Second dense layer\n",
        "    layers.Dropout(0.5),  # Dropout for regularization\n",
        "    layers.Dense(train_data.num_classes, activation='softmax')  # Final output layer\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oF3pGLMR3ebX"
      },
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer=Adam(),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412
        },
        "id": "IBu6muxJ3h_-",
        "outputId": "27732892-3dc9-4cdb-ec45-d6b7767adc31"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_data,\n",
        "    validation_data=val_data,\n",
        "    epochs=10,  # Adjust the number of epochs as needed\n",
        "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0I9bajb3liK"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model on the test data\n",
        "test_loss, test_accuracy = model.evaluate(test_data)\n",
        "\n",
        "print(\"Test Loss:\", test_loss)\n",
        "print(\"Test Accuracy:\", test_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Jc1I--M3osL"
      },
      "outputs": [],
      "source": [
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Training and Validation Accuracy Over Epochs\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Save Model Results (Google Colab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###### Save all the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# save model by hdf5\n",
        "model.save('/content/sample_data/weather-dataset/vgg_model.h5')\n",
        "# resnet_model.save('/content/sample_data/weather-dataset/resnet_model.h5')\n",
        "# xception_model.save('/content/sample_data/weather-dataset/xception_model.h5')\n",
        "\n",
        "# save model in SavedModel format\n",
        "model.save('/content/sample_data/weather-dataset/vgg_model')\n",
        "# resnet_model.save('/content/sample_data/weather-dataset/resnet_model')\n",
        "# xception_model.save('/content/sample_data/weather-dataset/xception_model')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###### save weights of the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# save weights of the model\n",
        "\n",
        "model.save_weights('/content/sample_data/weather-dataset/vgg_weights.h5')\n",
        "# resnet_model.save_weights('/content/sample_data/weather-dataset/resnet_weights.h5')\n",
        "# xception_model.save_weights('/content/sample_data/weather-dataset/xception_weights.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###### save onley architecture to json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# save only architecture to_json\n",
        "\n",
        "models={\n",
        "     'vgg_model':model,\n",
        "     # 'resnet_model':resnet_model,\n",
        "     # 'xception_model':xception_model\n",
        "}\n",
        "\n",
        "json_string = {\n",
        "     'json_string_vgg':models['vgg_model'].to_json(),\n",
        "     # 'json_string_resnet':models['resnet_model'].to_json(),\n",
        "     # 'json_string_xception':models['xception_model'].to_json()\n",
        "}\n",
        "\n",
        "with open(\"/content/sample_data/weather-dataset/model_vgg\", \"w\") as f: \n",
        "     f.write(json_string['json_string_vgg'])\n",
        "# with open(\"/content/sample_data/weather-dataset/model_resnet\", \"w\") as f: \n",
        "#      f.write(json_string['json_string_resnet'])\n",
        "# with open(\"/content/sample_data/weather-dataset/model_xception\", \"w\") as f: \n",
        "#      f.write(json_string['json_string_xception'])\n",
        "\n",
        "\n",
        "with open(\"/content/sample_data/weather-dataset/model_vgg\", \"r\") as f: \n",
        "     loaded_json_string_vgg = f.read()\n",
        "# with open(\"/content/sample_data/weather-dataset/model_resnet\", \"r\") as f: \n",
        "#      loaded_json_string_resnet = f.read()\n",
        "# with open(\"/content/sample_data/weather-dataset/model_xception\", \"r\") as f: \n",
        "#      loaded_json_string_xception = f.read()\n",
        "\n",
        "\n",
        "new_model_vgg = keras.models.model_from_json(loaded_json_string_vgg) \n",
        "print(new_model_vgg.summary())\n",
        "\n",
        "# new_model_resnet = keras.models.model_from_json(loaded_json_string_resnet) \n",
        "# print(new_model_resnet.summary())\n",
        "\n",
        "# new_model_xception = keras.models.model_from_json(loaded_json_string_xception) \n",
        "# print(new_model_xception.summary())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
